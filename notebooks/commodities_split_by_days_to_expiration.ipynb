{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f81bbf3-104d-47f1-a5d5-3aeb1fdd1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analyze volume for all cattle commodities split by before and after an arbitrary number of days to contract expiration (DTE).\n",
    "For example if we set the DTE to 45 then in each chart would get one volume trend line showing volumes on trading contracts that are\n",
    "greater then 45 DTE and another for those with less than 45 DTE\n",
    "For each contract we plot the following:\n",
    "- Average Daily Nominal Trading Volume By Minute\n",
    "- Average Daily Normalized Trading Volume By Minute\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "055216a6-e91f-4e81-837b-ac3e93bfa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTS_PREFIX_MATCHER = 'LE' # Only contracts with filenames matching this prefix will be analyzed\n",
    "DAYS_TO_EXPIRATION_THRESHOLD = 45.0 # A positive float to use for splitting the volume data under analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff734b-6964-4130-93de-216c04de72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotly as the plotting engine for pandas for convenience\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cf0c21-d5b0-4b19-acd3-71c749dbcd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the following files: ['LEG09.csv', 'LEG10.csv', 'LEG11.csv', 'LEG12.csv', 'LEG13.csv', 'LEG14.csv', 'LEG15.csv', 'LEG16.csv', 'LEG17.csv', 'LEG18.csv', 'LEG19.csv', 'LEG20.csv', 'LEJ08.csv', 'LEJ09.csv', 'LEJ10.csv', 'LEJ11.csv', 'LEJ12.csv', 'LEJ13.csv', 'LEJ14.csv', 'LEJ15.csv', 'LEJ16.csv', 'LEJ17.csv', 'LEJ18.csv', 'LEJ19.csv', 'LEJ20.csv', 'LEM08.csv', 'LEM09.csv', 'LEM10.csv', 'LEM11.csv', 'LEM12.csv', 'LEM13.csv', 'LEM14.csv', 'LEM15.csv', 'LEM16.csv', 'LEM17.csv', 'LEM18.csv', 'LEM19.csv', 'LEM20.csv', 'LEQ08.csv', 'LEQ09.csv', 'LEQ10.csv', 'LEQ11.csv', 'LEQ12.csv', 'LEQ13.csv', 'LEQ14.csv', 'LEQ15.csv', 'LEQ16.csv', 'LEQ17.csv', 'LEQ18.csv', 'LEQ19.csv', 'LEQ20.csv', 'LEV08.csv', 'LEV09.csv', 'LEV10.csv', 'LEV11.csv', 'LEV12.csv', 'LEV13.csv', 'LEV14.csv', 'LEV15.csv', 'LEV16.csv', 'LEV17.csv', 'LEV18.csv', 'LEV19.csv', 'LEV20.csv', 'LEZ08.csv', 'LEZ09.csv', 'LEZ10.csv', 'LEZ11.csv', 'LEZ12.csv', 'LEZ13.csv', 'LEZ14.csv', 'LEZ15.csv', 'LEZ16.csv', 'LEZ17.csv', 'LEZ18.csv', 'LEZ19.csv', 'LEZ20.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the csv files to process\n",
    "csv_files = []\n",
    "for file in os.listdir(\"../data/raw/firstratedata_futures\"):\n",
    "    if file.startswith(CONTRACTS_PREFIX_MATCHER):\n",
    "        csv_files.append(file)\n",
    "csv_files.sort()\n",
    "print(f\"Analyzing the following files: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d178cab-184e-45f3-aaa8-fd19a6a45af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_df(filename):\n",
    "    df_volume = pd.read_csv(\n",
    "        f\"../data/raw/firstratedata_futures/{filename}\",\n",
    "        parse_dates=['DateTime'], usecols=['DateTime', 'Volume'], index_col=['DateTime']\n",
    "    )\n",
    "    return df_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f8b83-1395-47e9-9a28-8c221e9f08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_trading_days(df):\n",
    "    '''Calculate the number of unique trading days in the dataset'''\n",
    "    unique_trading_days = df.index.map(lambda t: t.date()).unique()\n",
    "    return unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fd9cb0-953b-4663-9993-73b0aa861b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_df_grouped_by_minute():\n",
    "    '''Initialize an empty dataframe with no data and an index with a row for each minute of the day'''\n",
    "    date_range = pd.date_range(start='1/1/2021', end='1/02/2021', freq='T')[:-1]\n",
    "    new_df = pd.DataFrame(data={'DateTime':date_range}).set_index('DateTime')\n",
    "    new_df = new_df.groupby(lambda x: x.time()).sum()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b64e3c-fcf7-4079-8046-1b6217ad65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_frames(df1,df2):\n",
    "    '''Return a dataframe that concats the two provided dataframes together'''\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "717dc515-7107-4359-a2b0-7cad0bfbd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_volume_by_minute(df):\n",
    "    '''Resample the data-set by minute filling in the gaps and summing the trading volume within each minute'''\n",
    "    df_temp = df[['Volume']].resample('1T').sum()[[\"Volume\"]]\n",
    "    df_volume_by_minute = df_temp.groupby(lambda x: x.time()).sum()\n",
    "    return df_volume_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c08a19ad-f390-4491-9f49-d7f7f42deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_dte(a_df: pd.DataFrame, dte_threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "    Split a dataframe into two dataframes. One contains all rows where the dte is <= the dte_threshold \n",
    "    and the other contains all rows where the dte is > the dte_threshold\n",
    "    '''\n",
    "    less_than_or_equal_dte_threshold_df = a_df[a_df[\"Days To Contract Expiration\"] <= dte_threshold].copy()\n",
    "    greater_than_dte_threshold_df = a_df[a_df[\"Days To Contract Expiration\"] > dte_threshold].copy()\n",
    "    return (less_than_or_equal_dte_threshold_df, greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5668dd1e-7f6f-4b9b-a759-8e9687e2748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_avg_daily_nominal_df(\n",
    "    lte_dte_by_minute_df: pd.DataFrame, gt_dte_by_minute_df: pd.DataFrame,\n",
    "    num_lte_dte_unique_trading_days: int, num_gt_dte_unique_trading_days: int\n",
    ") -> pd.DataFrame:\n",
    "    '''Create the dataframe with the average intraday nominal trading volume by minute'''\n",
    "    master_avg_daily_nominal_df = initialize_df_grouped_by_minute()\n",
    "    lte_dte_by_minute_df = lte_dte_by_minute_df.rename(columns={'Volume':f\"Total Volume <= {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "    lte_dte_by_minute_df[f\"Average Volume <= {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"] = lte_dte_by_minute_df.apply(lambda row: row / num_lte_dte_unique_trading_days )\n",
    "    gt_dte_by_minute_df = gt_dte_by_minute_df.rename(columns={'Volume':f\"Total Volume > {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "    gt_dte_by_minute_df[f\"Average Volume > {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"] = gt_dte_by_minute_df.apply(lambda row: row / num_gt_dte_unique_trading_days )\n",
    "    master_avg_daily_nominal_df = pd.concat([master_avg_daily_nominal_df, lte_dte_by_minute_df, gt_dte_by_minute_df], axis=1)\n",
    "    return master_avg_daily_nominal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e185f790-1b17-4a37-9b41-56074c12253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_master_avg_daily_nominal_df(master_avg_daily_nominal_df: pd.DataFrame, split_date_cutoff: int) -> pd.DataFrame:\n",
    "    '''Drop columns from the df we don't need'''\n",
    "    return master_avg_daily_nominal_df.drop([f\"Total Volume Before {split_date_cutoff}\",f\"Total Volume After {split_date_cutoff}\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85600c6-933e-43ef-9f34-c5d58e4e87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_master_avg_daily_normalized_df(df_to_clean: pd.DataFrame, split_date_cutoff: int) -> pd.DataFrame:\n",
    "    '''Drop columns from the df we don't need'''\n",
    "    return df_to_clean.drop([f\"Total Volume Before {split_date_cutoff}\",f\"Total Volume After {split_date_cutoff}\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d6430d-fc83-4eb6-aaa9-25c1fb1901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of normalized values given an ndarray of nominal values\n",
    "def normalize_nd_array(to_normalize):\n",
    "    '''train the normalization'''\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(to_normalize)\n",
    "    # print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "    # normalize the dataset and print the first 5 rows\n",
    "    normalized = scaler.transform(to_normalize)\n",
    "    normalized = list(map(lambda x: x[0], normalized.tolist()))\n",
    "    # for i in range(5):\n",
    "    # \tprint(normalized[i])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ca5e69-b4fb-4c17-973b-d3995fdcb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes_for_volume_grouped_by_minute(df_volume_by_minute, unique_trading_days):\n",
    "    '''\n",
    "    Create and populate an array of dataframes. Each dataframe contains one trading days worth of volume values grouped by minute\n",
    "    and normalized against just that days worth of activity\n",
    "    '''\n",
    "    frames = []\n",
    "    for i in trange(len(unique_trading_days), desc=\"Splitting into dataframes grouped by minute\"):\n",
    "        day=unique_trading_days[i]\n",
    "        string_date = day.strftime(\"%Y-%m-%d\")\n",
    "        days_df = df_volume_by_minute.loc[string_date]\n",
    "        volume_values = days_df['Volume'].values\n",
    "        volume_values = volume_values.reshape((len(volume_values), 1))\n",
    "        normalized_day_volume = normalize_nd_array(volume_values)\n",
    "        days_df['Volume Normalized Intraday'] = normalized_day_volume\n",
    "        frames.append(days_df.copy())\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfb4991-64ff-4d0a-b392-edb71ecad5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_single_df(frames):\n",
    "    '''Concatenate the entire array of dataframes back into one big dataframe that contains the Volume Normalized Intraday values for every minute of every day'''\n",
    "    df_intraday_normalized = pd.concat(frames)\n",
    "    return df_intraday_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2343869e-1c4c-4c59-839e-8667b435279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the volume Normalized intraday Avg\n",
    "def calculate_normalized_vol_by_minute(intraday_summed, num_unique_trading_days):\n",
    "    return intraday_summed / num_unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "210f867b-9783-466e-a687-ec1068bab67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_minute_sum_normalized_volumes(df_intraday_normalized, num_unique_trading_days):\n",
    "    '''Group by minute across all days summing the intraday normalized volumes'''\n",
    "    df_normalized_grouped_by_minute = df_intraday_normalized.groupby(lambda x: x.time()).sum().rename(columns={'Volume Normalized Intraday':'Volume Normalized Summed'})\n",
    "    # Add a column that shows the average intraday normalized volume for each minute\n",
    "    df_normalized_grouped_by_minute['Daily Avg Volume Normalized'] = df_normalized_grouped_by_minute.apply(\n",
    "        lambda row: calculate_normalized_vol_by_minute(row['Volume Normalized Summed'],\n",
    "                                                       num_unique_trading_days),\n",
    "        axis=1\n",
    "    )\n",
    "    return df_normalized_grouped_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb243628-b227-47ef-84ee-d83bce2cb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dte_for_row(row, last_unique_trading_minute_in_contract):\n",
    "    this_row_date = row.name.to_pydatetime()\n",
    "    # print(f\"this_row_date {this_row_date}\")\n",
    "    time_difference = last_unique_trading_minute_in_contract - this_row_date\n",
    "    # print(f\"time_difference {time_difference}\")\n",
    "    return time_difference.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf706c3-b8c2-4ed3-9329-580bc01ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dte_column_to_df(a_contract_df):\n",
    "    unique_trading_days = list(get_unique_trading_days(a_contract_df))\n",
    "    unique_trading_days.sort()\n",
    "    last_unique_trading_day_in_contract = unique_trading_days[-1].strftime(\"%Y-%m-%d\")\n",
    "    last_unique_trading_minute_in_contract = a_contract_df.loc[last_unique_trading_day_in_contract].iloc[-1].name.to_pydatetime()\n",
    "    a_contract_df[\"Days To Contract Expiration\"] = a_contract_df.apply(lambda r: calculate_dte_for_row(r, last_unique_trading_minute_in_contract), axis=1)\n",
    "    return a_contract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba44504-604c-4737-b75a-633cb6ea4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_ungrouped_data_frame(files_to_process):\n",
    "    '''Build up a single dataframe containing volume DateTime and DTE for all contracts'''\n",
    "    grouped_df = pd.DataFrame(columns = [\"DateTime\", \"Volume\"]).set_index('DateTime')\n",
    "    for i in trange(len(files_to_process), desc=f\"Overall Analysis\"):\n",
    "        file = files_to_process[i]\n",
    "        contract_symbol = file[:len(file) - 4]\n",
    "        a_contract_df = convert_csv_to_df(file)\n",
    "        with_dte_df = add_dte_column_to_df(a_contract_df)\n",
    "        grouped_df = combine_data_frames(grouped_df, with_dte_df)\n",
    "        # display(a_contract_df)\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd260140-d363-490c-8f71-30b86327b19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e55adf714342da849dd3e736741767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Analysis:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather all the data from every contract into one big dataframe\n",
    "master_ungrouped_df = get_master_ungrouped_data_frame(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ace73875-61e8-49aa-b0bb-ebaa8b8aa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the grouped dataframe\n",
    "master_ungrouped_df = master_ungrouped_df.sort_values(by=['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a179e452-aacc-4a84-8782-22d26c8682c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe in two based on the DTE threshold\n",
    "less_than_or_equal_dte_threshold_df, greater_than_dte_threshold_df = split_dataframe_by_dte(master_ungrouped_df, DAYS_TO_EXPIRATION_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9ee4f98-c440-4c95-ac3c-344c7a86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the unique trading days before and after the DTE threshold\n",
    "less_than_or_equal_dte_unique_trading_days = get_unique_trading_days(less_than_or_equal_dte_threshold_df)\n",
    "greater_than_dte_unique_trading_days = get_unique_trading_days(greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9527a544-893f-4310-bc36-28d42e91a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique trading days before and after the cutoff date\n",
    "num_lte_dte_unique_trading_days = len(less_than_or_equal_dte_unique_trading_days)\n",
    "num_gt_dte_unique_trading_days = len(greater_than_dte_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69c0702f-670e-4bd8-91c4-05f76904b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the before and after dataframes to show the total volume by minute of the day\n",
    "lte_dte_by_minute_df = resample_volume_by_minute(less_than_or_equal_dte_threshold_df)\n",
    "gt_dte_by_minute_df = resample_volume_by_minute(greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20dae454-ba8c-40e0-beb6-034361997579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average intraday volume for each minute of the day before and after the dte threshold\n",
    "# Put all that information into one dataframe for charting\n",
    "master_avg_daily_nominal_df = get_master_avg_daily_nominal_df(\n",
    "    lte_dte_by_minute_df, gt_dte_by_minute_df,\n",
    "    num_lte_dte_unique_trading_days, num_gt_dte_unique_trading_days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "548fa2cf-e53b-4e9b-ba12-1c0bfd917cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Volume &lt;= 45.0 DTE</th>\n",
       "      <th>Average Volume &lt;= 45.0 DTE</th>\n",
       "      <th>Total Volume &gt; 45.0 DTE</th>\n",
       "      <th>Average Volume &gt; 45.0 DTE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>09:20:00</th>\n",
       "      <td>6581</td>\n",
       "      <td>2.576742</td>\n",
       "      <td>32434</td>\n",
       "      <td>9.547836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:21:00</th>\n",
       "      <td>6321</td>\n",
       "      <td>2.474941</td>\n",
       "      <td>31953</td>\n",
       "      <td>9.406241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:22:00</th>\n",
       "      <td>6518</td>\n",
       "      <td>2.552075</td>\n",
       "      <td>33112</td>\n",
       "      <td>9.747424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:23:00</th>\n",
       "      <td>6283</td>\n",
       "      <td>2.460063</td>\n",
       "      <td>33145</td>\n",
       "      <td>9.757139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:24:00</th>\n",
       "      <td>6046</td>\n",
       "      <td>2.367267</td>\n",
       "      <td>32507</td>\n",
       "      <td>9.569326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:25:00</th>\n",
       "      <td>6494</td>\n",
       "      <td>2.542678</td>\n",
       "      <td>30553</td>\n",
       "      <td>8.994112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:26:00</th>\n",
       "      <td>6595</td>\n",
       "      <td>2.582224</td>\n",
       "      <td>33038</td>\n",
       "      <td>9.725640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:27:00</th>\n",
       "      <td>6181</td>\n",
       "      <td>2.420125</td>\n",
       "      <td>31162</td>\n",
       "      <td>9.173388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:28:00</th>\n",
       "      <td>6009</td>\n",
       "      <td>2.352780</td>\n",
       "      <td>32018</td>\n",
       "      <td>9.425375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:29:00</th>\n",
       "      <td>6573</td>\n",
       "      <td>2.573610</td>\n",
       "      <td>33043</td>\n",
       "      <td>9.727112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:30:00</th>\n",
       "      <td>178733</td>\n",
       "      <td>69.981597</td>\n",
       "      <td>1284627</td>\n",
       "      <td>378.165146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:31:00</th>\n",
       "      <td>76935</td>\n",
       "      <td>30.123336</td>\n",
       "      <td>567127</td>\n",
       "      <td>166.949367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:32:00</th>\n",
       "      <td>59154</td>\n",
       "      <td>23.161316</td>\n",
       "      <td>452152</td>\n",
       "      <td>133.103326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:33:00</th>\n",
       "      <td>52323</td>\n",
       "      <td>20.486688</td>\n",
       "      <td>388367</td>\n",
       "      <td>114.326465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:34:00</th>\n",
       "      <td>46396</td>\n",
       "      <td>18.166014</td>\n",
       "      <td>332489</td>\n",
       "      <td>97.877245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:35:00</th>\n",
       "      <td>47809</td>\n",
       "      <td>18.719264</td>\n",
       "      <td>340534</td>\n",
       "      <td>100.245511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:36:00</th>\n",
       "      <td>41325</td>\n",
       "      <td>16.180501</td>\n",
       "      <td>298845</td>\n",
       "      <td>87.973212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:37:00</th>\n",
       "      <td>37518</td>\n",
       "      <td>14.689898</td>\n",
       "      <td>278041</td>\n",
       "      <td>81.848984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:38:00</th>\n",
       "      <td>37028</td>\n",
       "      <td>14.498042</td>\n",
       "      <td>267203</td>\n",
       "      <td>78.658522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09:39:00</th>\n",
       "      <td>34875</td>\n",
       "      <td>13.655051</td>\n",
       "      <td>247232</td>\n",
       "      <td>72.779511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total Volume <= 45.0 DTE  Average Volume <= 45.0 DTE  \\\n",
       "DateTime                                                         \n",
       "09:20:00                      6581                    2.576742   \n",
       "09:21:00                      6321                    2.474941   \n",
       "09:22:00                      6518                    2.552075   \n",
       "09:23:00                      6283                    2.460063   \n",
       "09:24:00                      6046                    2.367267   \n",
       "09:25:00                      6494                    2.542678   \n",
       "09:26:00                      6595                    2.582224   \n",
       "09:27:00                      6181                    2.420125   \n",
       "09:28:00                      6009                    2.352780   \n",
       "09:29:00                      6573                    2.573610   \n",
       "09:30:00                    178733                   69.981597   \n",
       "09:31:00                     76935                   30.123336   \n",
       "09:32:00                     59154                   23.161316   \n",
       "09:33:00                     52323                   20.486688   \n",
       "09:34:00                     46396                   18.166014   \n",
       "09:35:00                     47809                   18.719264   \n",
       "09:36:00                     41325                   16.180501   \n",
       "09:37:00                     37518                   14.689898   \n",
       "09:38:00                     37028                   14.498042   \n",
       "09:39:00                     34875                   13.655051   \n",
       "\n",
       "          Total Volume > 45.0 DTE  Average Volume > 45.0 DTE  \n",
       "DateTime                                                      \n",
       "09:20:00                    32434                   9.547836  \n",
       "09:21:00                    31953                   9.406241  \n",
       "09:22:00                    33112                   9.747424  \n",
       "09:23:00                    33145                   9.757139  \n",
       "09:24:00                    32507                   9.569326  \n",
       "09:25:00                    30553                   8.994112  \n",
       "09:26:00                    33038                   9.725640  \n",
       "09:27:00                    31162                   9.173388  \n",
       "09:28:00                    32018                   9.425375  \n",
       "09:29:00                    33043                   9.727112  \n",
       "09:30:00                  1284627                 378.165146  \n",
       "09:31:00                   567127                 166.949367  \n",
       "09:32:00                   452152                 133.103326  \n",
       "09:33:00                   388367                 114.326465  \n",
       "09:34:00                   332489                  97.877245  \n",
       "09:35:00                   340534                 100.245511  \n",
       "09:36:00                   298845                  87.973212  \n",
       "09:37:00                   278041                  81.848984  \n",
       "09:38:00                   267203                  78.658522  \n",
       "09:39:00                   247232                  72.779511  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master_avg_daily_nominal_df.iloc[560:580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4355a-1576-4eae-a0bf-68c76e346f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from the dataframe that we have no intention of charting\n",
    "master_avg_daily_nominal_df = clean_master_avg_daily_nominal_df(\n",
    "    master_avg_daily_nominal_df, DAYS_TO_EXPIRATION_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6f3e-745f-4073-a20c-725a5f0eb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two arrays of dataframes each array element is a dataframe for a single days worth of intraday normalized volume\n",
    "before_cutoff_date_frames = create_dataframes_for_volume_grouped_by_minute(master_ungrouped_df, before_date_unique_trading_days)\n",
    "after_cutoff_date_frames = create_dataframes_for_volume_grouped_by_minute(master_ungrouped_df, after_date_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1674c44-2c62-4b7a-87b6-aa9a9db469e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the array of dataframes into one big dataframe each for before and after the cutoff date\n",
    "df_intraday_normalized_before = concat_to_single_df(before_cutoff_date_frames)\n",
    "df_intraday_normalized_after = concat_to_single_df(after_cutoff_date_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e852-1af5-4e98-a4aa-b62c170f0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intraday_normalized_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e34a-b382-45da-b8f2-43596296570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by minute normalized for both before and after the cutoff date\n",
    "df_intraday_normalized_before_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_before, num_before_date_unique_trading_days)\n",
    "df_intraday_normalized_after_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_after, num_after_date_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc114f45-576d-4077-b728-39251437fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop some columns we no longer need\n",
    "df_intraday_normalized_before_grouped_by_minute = df_intraday_normalized_before_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume Before {DAYS_TO_EXPIRATION_THRESHOLD}\"})\n",
    "df_intraday_normalized_before_grouped_by_minute = df_intraday_normalized_before_grouped_by_minute.drop([\"Volume Normalized Summed\"], axis=1)\n",
    "df_intraday_normalized_after_grouped_by_minute = df_intraday_normalized_after_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume After {DAYS_TO_EXPIRATION_THRESHOLD}\"})\n",
    "df_intraday_normalized_after_grouped_by_minute = df_intraday_normalized_after_grouped_by_minute.drop([\"Volume Normalized Summed\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24ce22-eb76-49fd-b073-d55cb4a18dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all relev\n",
    "master_avg_daily_normalized_df = pd.concat([df_intraday_normalized_before_grouped_by_minute, df_intraday_normalized_after_grouped_by_minute], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540059-a4e0-4952-9e29-35075dba140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display our figure for nominal intraday volume\n",
    "fig1 = master_avg_daily_nominal_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Nominal Trading Volume By Minute\")\n",
    "fig2 = master_avg_daily_normalized_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Normalized Trading Volume By Minute\")\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25f525-0dd4-4195-bd95-06a0365c14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
