{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f81bbf3-104d-47f1-a5d5-3aeb1fdd1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analyze volume for all cattle commodities split by before and after an arbitrary number of days to contract expiration (DTE).\n",
    "For example if we set the DTE to 45 then in each chart would get one volume trend line showing volumes on trading contracts that are\n",
    "greater then 45 DTE and another for those with less than 45 DTE\n",
    "For each contract we plot the following:\n",
    "- Average Daily Nominal Trading Volume By Minute\n",
    "- Average Daily Normalized Trading Volume By Minute\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055216a6-e91f-4e81-837b-ac3e93bfa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTS_PREFIX_MATCHER = 'LE' # Only contracts with filenames matching this prefix will be analyzed\n",
    "DAYS_TO_EXPIRATION_CUTOFF = '2018-06-07' # A positive integer to use for splitting the volume data under analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff734b-6964-4130-93de-216c04de72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotly as the plotting engine for pandas for convenience\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cf0c21-d5b0-4b19-acd3-71c749dbcd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the following files: ['LEG09.csv', 'LEG10.csv', 'LEG11.csv', 'LEG12.csv', 'LEG13.csv', 'LEG14.csv', 'LEG15.csv', 'LEG16.csv', 'LEG17.csv', 'LEG18.csv', 'LEG19.csv', 'LEG20.csv', 'LEJ08.csv', 'LEJ09.csv', 'LEJ10.csv', 'LEJ11.csv', 'LEJ12.csv', 'LEJ13.csv', 'LEJ14.csv', 'LEJ15.csv', 'LEJ16.csv', 'LEJ17.csv', 'LEJ18.csv', 'LEJ19.csv', 'LEJ20.csv', 'LEM08.csv', 'LEM09.csv', 'LEM10.csv', 'LEM11.csv', 'LEM12.csv', 'LEM13.csv', 'LEM14.csv', 'LEM15.csv', 'LEM16.csv', 'LEM17.csv', 'LEM18.csv', 'LEM19.csv', 'LEM20.csv', 'LEQ08.csv', 'LEQ09.csv', 'LEQ10.csv', 'LEQ11.csv', 'LEQ12.csv', 'LEQ13.csv', 'LEQ14.csv', 'LEQ15.csv', 'LEQ16.csv', 'LEQ17.csv', 'LEQ18.csv', 'LEQ19.csv', 'LEQ20.csv', 'LEV08.csv', 'LEV09.csv', 'LEV10.csv', 'LEV11.csv', 'LEV12.csv', 'LEV13.csv', 'LEV14.csv', 'LEV15.csv', 'LEV16.csv', 'LEV17.csv', 'LEV18.csv', 'LEV19.csv', 'LEV20.csv', 'LEZ08.csv', 'LEZ09.csv', 'LEZ10.csv', 'LEZ11.csv', 'LEZ12.csv', 'LEZ13.csv', 'LEZ14.csv', 'LEZ15.csv', 'LEZ16.csv', 'LEZ17.csv', 'LEZ18.csv', 'LEZ19.csv', 'LEZ20.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the csv files to process\n",
    "csv_files = []\n",
    "for file in os.listdir(\"../data/raw/firstratedata_futures\"):\n",
    "    if file.startswith(CONTRACTS_PREFIX_MATCHER):\n",
    "        csv_files.append(file)\n",
    "csv_files.sort()\n",
    "print(f\"Analyzing the following files: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d178cab-184e-45f3-aaa8-fd19a6a45af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_df(filename):\n",
    "    df_volume = pd.read_csv(\n",
    "        f\"../data/raw/firstratedata_futures/{filename}\",\n",
    "        parse_dates=['DateTime'], usecols=['DateTime', 'Volume'], index_col=['DateTime']\n",
    "    )\n",
    "    return df_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f8b83-1395-47e9-9a28-8c221e9f08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_trading_days(df):\n",
    "    '''Calculate the number of unique trading days in the dataset'''\n",
    "    unique_trading_days = df.index.map(lambda t: t.date()).unique()\n",
    "    return unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fd9cb0-953b-4663-9993-73b0aa861b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_df_grouped_by_minute():\n",
    "    '''Initialize an empty dataframe with no data and an index with a row for each minute of the day'''\n",
    "    date_range = pd.date_range(start='1/1/2021', end='1/02/2021', freq='T')[:-1]\n",
    "    new_df = pd.DataFrame(data={'DateTime':date_range}).set_index('DateTime')\n",
    "    new_df = new_df.groupby(lambda x: x.time()).sum()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b64e3c-fcf7-4079-8046-1b6217ad65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_frames(df1,df2):\n",
    "    '''Return a dataframe that concats the two provided dataframes together'''\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717dc515-7107-4359-a2b0-7cad0bfbd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_volume_by_minute(df):\n",
    "    '''Resample the data-set by minute filling in the gaps and summing the trading volume within each minute'''\n",
    "    df_temp = df.resample('1T').sum()[[\"Volume\"]]\n",
    "    df_volume_by_minute = df_temp.groupby(lambda x: x.time()).sum()\n",
    "    return df_volume_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2608e6a-3e14-4aaa-814b-aea1d9471a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_date(df, split_date):\n",
    "    '''Split a dataframe into two dataframes. Onecontains all rows before the split_date and the other contains all rows after it'''\n",
    "    before_date_df = master_ungrouped_df[master_ungrouped_df.index.date < datetime.strptime(split_date, '%Y-%m-%d').date()].copy()\n",
    "    after_date_df = master_ungrouped_df[master_ungrouped_df.index.date >= datetime.strptime(split_date, '%Y-%m-%d').date()].copy()\n",
    "    return (before_date_df, after_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5668dd1e-7f6f-4b9b-a759-8e9687e2748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_avg_daily_nominal_df(\n",
    "    before_date_by_minute_df: pd.DataFrame, after_date_by_minute_df: pd.DataFrame,\n",
    "    num_before_date_unique_trading_days: int, num_after_date_unique_trading_days: int\n",
    ") -> pd.DataFrame:\n",
    "    '''Create the dataframe with the average intraday nominal trading volume by minute'''\n",
    "    master_avg_daily_nominal_df = initialize_df_grouped_by_minute()\n",
    "    before_date_by_minute_df = before_date_by_minute_df.rename(columns={'Volume':f\"Total Volume Before {SPLIT_DATE_CUTOFF}\"})\n",
    "    before_date_by_minute_df[f\"Average Volume Before {SPLIT_DATE_CUTOFF}\"] = before_date_by_minute_df.apply(lambda row: row / num_before_date_unique_trading_days )\n",
    "    after_date_by_minute_df = after_date_by_minute_df.rename(columns={'Volume':f\"Total Volume After {SPLIT_DATE_CUTOFF}\"})\n",
    "    after_date_by_minute_df[f\"Average Volume After {SPLIT_DATE_CUTOFF}\"] = after_date_by_minute_df.apply(lambda row: row / num_after_date_unique_trading_days )\n",
    "    master_avg_daily_nominal_df = pd.concat([master_avg_daily_nominal_df, before_date_by_minute_df, after_date_by_minute_df], axis=1)\n",
    "    return master_avg_daily_nominal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e185f790-1b17-4a37-9b41-56074c12253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_master_avg_daily_nominal_df(master_avg_daily_nominal_df: pd.DataFrame, split_date_cutoff: int) -> pd.DataFrame:\n",
    "    '''Drop columns from the df we don't need'''\n",
    "    return master_avg_daily_nominal_df.drop([f\"Total Volume Before {split_date_cutoff}\",f\"Total Volume After {split_date_cutoff}\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85600c6-933e-43ef-9f34-c5d58e4e87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_master_avg_daily_normalized_df(df_to_clean: pd.DataFrame, split_date_cutoff: int) -> pd.DataFrame:\n",
    "    '''Drop columns from the df we don't need'''\n",
    "    return df_to_clean.drop([f\"Total Volume Before {split_date_cutoff}\",f\"Total Volume After {split_date_cutoff}\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d6430d-fc83-4eb6-aaa9-25c1fb1901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of normalized values given an ndarray of nominal values\n",
    "def normalize_nd_array(to_normalize):\n",
    "    '''train the normalization'''\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(to_normalize)\n",
    "    # print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "    # normalize the dataset and print the first 5 rows\n",
    "    normalized = scaler.transform(to_normalize)\n",
    "    normalized = list(map(lambda x: x[0], normalized.tolist()))\n",
    "    # for i in range(5):\n",
    "    # \tprint(normalized[i])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ca5e69-b4fb-4c17-973b-d3995fdcb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes_for_volume_grouped_by_minute(df_volume_by_minute, unique_trading_days):\n",
    "    '''\n",
    "    Create and populate an array of dataframes. Each dataframe contains one trading days worth of volume values grouped by minute\n",
    "    and normalized against just that days worth of activity\n",
    "    '''\n",
    "    frames = []\n",
    "    for i in trange(len(unique_trading_days), desc=\"Splitting into dataframes grouped by minute\"):\n",
    "        day=unique_trading_days[i]\n",
    "        string_date = day.strftime(\"%Y-%m-%d\")\n",
    "        days_df = df_volume_by_minute.loc[string_date]\n",
    "        volume_values = days_df['Volume'].values\n",
    "        volume_values = volume_values.reshape((len(volume_values), 1))\n",
    "        normalized_day_volume = normalize_nd_array(volume_values)\n",
    "        days_df['Volume Normalized Intraday'] = normalized_day_volume\n",
    "        frames.append(days_df.copy())\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfb4991-64ff-4d0a-b392-edb71ecad5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_single_df(frames):\n",
    "    '''Concatenate the entire array of dataframes back into one big dataframe that contains the Volume Normalized Intraday values for every minute of every day'''\n",
    "    df_intraday_normalized = pd.concat(frames)\n",
    "    return df_intraday_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2343869e-1c4c-4c59-839e-8667b435279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the volume Normalized intraday Avg\n",
    "def calculate_normalized_vol_by_minute(intraday_summed, num_unique_trading_days):\n",
    "    return intraday_summed / num_unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "210f867b-9783-466e-a687-ec1068bab67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_minute_sum_normalized_volumes(df_intraday_normalized, num_unique_trading_days):\n",
    "    '''Group by minute across all days summing the intraday normalized volumes'''\n",
    "    df_normalized_grouped_by_minute = df_intraday_normalized.groupby(lambda x: x.time()).sum().rename(columns={'Volume Normalized Intraday':'Volume Normalized Summed'})\n",
    "    # Add a column that shows the average intraday normalized volume for each minute\n",
    "    df_normalized_grouped_by_minute['Daily Avg Volume Normalized'] = df_normalized_grouped_by_minute.apply(\n",
    "        lambda row: calculate_normalized_vol_by_minute(row['Volume Normalized Summed'],\n",
    "                                                       num_unique_trading_days),\n",
    "        axis=1\n",
    "    )\n",
    "    return df_normalized_grouped_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb243628-b227-47ef-84ee-d83bce2cb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dte_for_row(row, last_unique_trading_minute_in_contract):\n",
    "    this_row_date = row.name.to_pydatetime()\n",
    "    # print(f\"this_row_date {this_row_date}\")\n",
    "    time_difference = last_unique_trading_minute_in_contract - this_row_date\n",
    "    # print(f\"time_difference {time_difference}\")\n",
    "    return time_difference.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf706c3-b8c2-4ed3-9329-580bc01ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dte_column_to_df(a_contract_df):\n",
    "    unique_trading_days = list(get_unique_trading_days(a_contract_df))\n",
    "    unique_trading_days.sort()\n",
    "    last_unique_trading_day_in_contract = unique_trading_days[-1].strftime(\"%Y-%m-%d\")\n",
    "    last_unique_trading_minute_in_contract = a_contract_df.loc[last_unique_trading_day_in_contract].iloc[-1].name.to_pydatetime()\n",
    "    a_contract_df[\"Days To Contract Expiration\"] = a_contract_df.apply(lambda r: calculate_dte_for_row(r, last_unique_trading_minute_in_contract), axis=1)\n",
    "    return a_contract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba44504-604c-4737-b75a-633cb6ea4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_ungrouped_data_frame(files_to_process):\n",
    "    '''Build up a single dataframe containing volume DateTime and DTE for all contracts'''\n",
    "    grouped_df = pd.DataFrame(columns = [\"DateTime\", \"Volume\"]).set_index('DateTime')\n",
    "    for i in trange(len(files_to_process), desc=f\"Overall Analysis\"):\n",
    "        file = files_to_process[i]\n",
    "        contract_symbol = file[:len(file) - 4]\n",
    "        a_contract_df = convert_csv_to_df(file)\n",
    "        with_dte_df = add_dte_column_to_df(a_contract_df)\n",
    "        grouped_df = combine_data_frames(grouped_df, with_dte_df)\n",
    "        # display(a_contract_df)\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd260140-d363-490c-8f71-30b86327b19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e55adf714342da849dd3e736741767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Analysis:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather all the data from every contract into one big dataframe\n",
    "master_ungrouped_df = get_master_ungrouped_data_frame(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ca0ce17-0a42-45df-a4f2-cf768aa482ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume</th>\n",
       "      <th>Days To Contract Expiration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-12 10:45:00</th>\n",
       "      <td>1</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12 10:45:00</th>\n",
       "      <td>70</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12 10:45:00</th>\n",
       "      <td>3</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12 10:45:00</th>\n",
       "      <td>31</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-12 10:45:00</th>\n",
       "      <td>13</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Volume  Days To Contract Expiration\n",
       "DateTime                                               \n",
       "2018-07-12 10:45:00      1                        231.0\n",
       "2018-07-12 10:45:00     70                         50.0\n",
       "2018-07-12 10:45:00      3                        414.0\n",
       "2018-07-12 10:45:00     31                        111.0\n",
       "2018-07-12 10:45:00     13                        172.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_ungrouped_df.loc['2018-07-12 10:45:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59673499-17bc-4d67-ad0f-e3f1dcf9704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort then split the big ungrouped dataframe into a before and after dataframe using the cutoff date\n",
    "master_ungrouped_df = master_ungrouped_df.sort_values(by=['DateTime'])\n",
    "before_date_df, after_date_df = split_dataframe_by_date(master_ungrouped_df, SPLIT_DATE_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee4f98-c440-4c95-ac3c-344c7a86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the unique trading days before and after the cutoff date\n",
    "before_date_unique_trading_days = get_unique_trading_days(before_date_df)\n",
    "after_date_unique_trading_days = get_unique_trading_days(after_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527a544-893f-4310-bc36-28d42e91a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique trading days before and after the cutoff date\n",
    "num_before_date_unique_trading_days = len(before_date_unique_trading_days)\n",
    "num_after_date_unique_trading_days = len(after_date_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0702f-670e-4bd8-91c4-05f76904b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the before and after dataframes to show the total volume by minute of the day\n",
    "before_date_by_minute_df = resample_volume_by_minute(before_date_df)\n",
    "after_date_by_minute_df = resample_volume_by_minute(after_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dae454-ba8c-40e0-beb6-034361997579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average intraday volume for each minute of the day before and after the cutoff date.\n",
    "# Put all that information into one dataframe for charting\n",
    "master_avg_daily_nominal_df = get_master_avg_daily_nominal_df(\n",
    "    before_date_by_minute_df, after_date_by_minute_df,\n",
    "    num_before_date_unique_trading_days, num_after_date_unique_trading_days\n",
    ")\n",
    "# master_avg_daily_nominal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4355a-1576-4eae-a0bf-68c76e346f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from the dataframe that we have no intention of charting\n",
    "master_avg_daily_nominal_df = clean_master_avg_daily_nominal_df(\n",
    "    master_avg_daily_nominal_df, SPLIT_DATE_CUTOFF\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6f3e-745f-4073-a20c-725a5f0eb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two arrays of dataframes each array element is a dataframe for a single days worth of intraday normalized volume\n",
    "before_cutoff_date_frames = create_dataframes_for_volume_grouped_by_minute(master_ungrouped_df, before_date_unique_trading_days)\n",
    "after_cutoff_date_frames = create_dataframes_for_volume_grouped_by_minute(master_ungrouped_df, after_date_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1674c44-2c62-4b7a-87b6-aa9a9db469e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the array of dataframes into one big dataframe each for before and after the cutoff date\n",
    "df_intraday_normalized_before = concat_to_single_df(before_cutoff_date_frames)\n",
    "df_intraday_normalized_after = concat_to_single_df(after_cutoff_date_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154e852-1af5-4e98-a4aa-b62c170f0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intraday_normalized_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e34a-b382-45da-b8f2-43596296570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by minute normalized for both before and after the cutoff date\n",
    "df_intraday_normalized_before_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_before, num_before_date_unique_trading_days)\n",
    "df_intraday_normalized_after_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_after, num_after_date_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc114f45-576d-4077-b728-39251437fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop some columns we no longer need\n",
    "df_intraday_normalized_before_grouped_by_minute = df_intraday_normalized_before_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume Before {SPLIT_DATE_CUTOFF}\"})\n",
    "df_intraday_normalized_before_grouped_by_minute = df_intraday_normalized_before_grouped_by_minute.drop([\"Volume Normalized Summed\"], axis=1)\n",
    "df_intraday_normalized_after_grouped_by_minute = df_intraday_normalized_after_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume After {SPLIT_DATE_CUTOFF}\"})\n",
    "df_intraday_normalized_after_grouped_by_minute = df_intraday_normalized_after_grouped_by_minute.drop([\"Volume Normalized Summed\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24ce22-eb76-49fd-b073-d55cb4a18dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all relev\n",
    "master_avg_daily_normalized_df = pd.concat([df_intraday_normalized_before_grouped_by_minute, df_intraday_normalized_after_grouped_by_minute], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540059-a4e0-4952-9e29-35075dba140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display our figure for nominal intraday volume\n",
    "fig1 = master_avg_daily_nominal_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Nominal Trading Volume By Minute\")\n",
    "fig2 = master_avg_daily_normalized_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Normalized Trading Volume By Minute\")\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25f525-0dd4-4195-bd95-06a0365c14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
