{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f81bbf3-104d-47f1-a5d5-3aeb1fdd1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analyze volume for all cattle commodities split by before and after an arbitrary number of days to contract expiration (DTE).\n",
    "For example if we set the DTE to 45 then in each chart would get one volume trend line showing volumes on trading contracts that are\n",
    "greater then 45 DTE and another for those with less than 45 DTE\n",
    "For each contract we plot the following:\n",
    "- Average Daily Nominal Trading Volume By Minute\n",
    "- Average Daily Normalized Trading Volume By Minute\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055216a6-e91f-4e81-837b-ac3e93bfa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTS_PREFIX_MATCHER = 'LE' # Only contracts with filenames matching this prefix will be analyzed\n",
    "DAYS_TO_EXPIRATION_THRESHOLD = 45.0 # A positive float to use for splitting the volume data under analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff734b-6964-4130-93de-216c04de72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotly as the plotting engine for pandas for convenience\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cf0c21-d5b0-4b19-acd3-71c749dbcd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the following files: ['LEG09.csv', 'LEG10.csv', 'LEG11.csv', 'LEG12.csv', 'LEG13.csv', 'LEG14.csv', 'LEG15.csv', 'LEG16.csv', 'LEG17.csv', 'LEG18.csv', 'LEG19.csv', 'LEG20.csv', 'LEJ08.csv', 'LEJ09.csv', 'LEJ10.csv', 'LEJ11.csv', 'LEJ12.csv', 'LEJ13.csv', 'LEJ14.csv', 'LEJ15.csv', 'LEJ16.csv', 'LEJ17.csv', 'LEJ18.csv', 'LEJ19.csv', 'LEJ20.csv', 'LEM08.csv', 'LEM09.csv', 'LEM10.csv', 'LEM11.csv', 'LEM12.csv', 'LEM13.csv', 'LEM14.csv', 'LEM15.csv', 'LEM16.csv', 'LEM17.csv', 'LEM18.csv', 'LEM19.csv', 'LEM20.csv', 'LEQ08.csv', 'LEQ09.csv', 'LEQ10.csv', 'LEQ11.csv', 'LEQ12.csv', 'LEQ13.csv', 'LEQ14.csv', 'LEQ15.csv', 'LEQ16.csv', 'LEQ17.csv', 'LEQ18.csv', 'LEQ19.csv', 'LEQ20.csv', 'LEV08.csv', 'LEV09.csv', 'LEV10.csv', 'LEV11.csv', 'LEV12.csv', 'LEV13.csv', 'LEV14.csv', 'LEV15.csv', 'LEV16.csv', 'LEV17.csv', 'LEV18.csv', 'LEV19.csv', 'LEV20.csv', 'LEZ08.csv', 'LEZ09.csv', 'LEZ10.csv', 'LEZ11.csv', 'LEZ12.csv', 'LEZ13.csv', 'LEZ14.csv', 'LEZ15.csv', 'LEZ16.csv', 'LEZ17.csv', 'LEZ18.csv', 'LEZ19.csv', 'LEZ20.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the csv files to process\n",
    "csv_files = []\n",
    "for file in os.listdir(\"../data/raw/firstratedata_futures\"):\n",
    "    if file.startswith(CONTRACTS_PREFIX_MATCHER):\n",
    "        csv_files.append(file)\n",
    "csv_files.sort()\n",
    "print(f\"Analyzing the following files: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d178cab-184e-45f3-aaa8-fd19a6a45af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_df(filename):\n",
    "    df_volume = pd.read_csv(\n",
    "        f\"../data/raw/firstratedata_futures/{filename}\",\n",
    "        parse_dates=['DateTime'], usecols=['DateTime', 'Volume'], index_col=['DateTime']\n",
    "    )\n",
    "    return df_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f8b83-1395-47e9-9a28-8c221e9f08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_trading_days(df):\n",
    "    '''Calculate the number of unique trading days in the dataset'''\n",
    "    unique_trading_days = df.index.map(lambda t: t.date()).unique()\n",
    "    return unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fd9cb0-953b-4663-9993-73b0aa861b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_df_grouped_by_minute():\n",
    "    '''Initialize an empty dataframe with no data and an index with a row for each minute of the day'''\n",
    "    date_range = pd.date_range(start='1/1/2021', end='1/02/2021', freq='T')[:-1]\n",
    "    new_df = pd.DataFrame(data={'DateTime':date_range}).set_index('DateTime')\n",
    "    new_df = new_df.groupby(lambda x: x.time()).sum()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b64e3c-fcf7-4079-8046-1b6217ad65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_frames(df1,df2):\n",
    "    '''Return a dataframe that concats the two provided dataframes together'''\n",
    "    combined_df = pd.concat([df1, df2])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717dc515-7107-4359-a2b0-7cad0bfbd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_volume_by_minute(df):\n",
    "    '''Resample the data-set by minute filling in the gaps and summing the trading volume within each minute'''\n",
    "    df_temp = df[['Volume']].resample('1T').sum()[[\"Volume\"]]\n",
    "    df_volume_by_minute = df_temp.groupby(lambda x: x.time()).sum()\n",
    "    return df_volume_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08a19ad-f390-4491-9f49-d7f7f42deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_dte(a_df: pd.DataFrame, dte_threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "    Split a dataframe into two dataframes. One contains all rows where the dte is <= the dte_threshold \n",
    "    and the other contains all rows where the dte is > the dte_threshold\n",
    "    '''\n",
    "    less_than_or_equal_dte_threshold_df = a_df[a_df[\"Days To Contract Expiration\"] <= dte_threshold].copy()\n",
    "    greater_than_dte_threshold_df = a_df[a_df[\"Days To Contract Expiration\"] > dte_threshold].copy()\n",
    "    return (less_than_or_equal_dte_threshold_df, greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5668dd1e-7f6f-4b9b-a759-8e9687e2748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_avg_daily_nominal_df(\n",
    "    lte_dte_by_minute_df: pd.DataFrame, gt_dte_by_minute_df: pd.DataFrame,\n",
    "    num_lte_dte_unique_trading_days: int, num_gt_dte_unique_trading_days: int\n",
    ") -> pd.DataFrame:\n",
    "    '''Create the dataframe with the average intraday nominal trading volume by minute'''\n",
    "    master_avg_daily_nominal_df = initialize_df_grouped_by_minute()\n",
    "    lte_dte_by_minute_df = lte_dte_by_minute_df.rename(columns={'Volume':f\"Total Volume <= {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "    lte_dte_by_minute_df[f\"Average Volume <= {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"] = lte_dte_by_minute_df.apply(lambda row: row / num_lte_dte_unique_trading_days )\n",
    "    gt_dte_by_minute_df = gt_dte_by_minute_df.rename(columns={'Volume':f\"Total Volume > {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "    gt_dte_by_minute_df[f\"Average Volume > {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"] = gt_dte_by_minute_df.apply(lambda row: row / num_gt_dte_unique_trading_days )\n",
    "    master_avg_daily_nominal_df = pd.concat([master_avg_daily_nominal_df, lte_dte_by_minute_df, gt_dte_by_minute_df], axis=1)\n",
    "    return master_avg_daily_nominal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e185f790-1b17-4a37-9b41-56074c12253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_master_avg_daily_nominal_df(master_avg_daily_nominal_df: pd.DataFrame, dte_volume_threshold: float) -> pd.DataFrame:\n",
    "    '''Drop columns from the df we don't need'''\n",
    "    return master_avg_daily_nominal_df.drop([f\"Total Volume <= {dte_volume_threshold} DTE\",f\"Total Volume > {dte_volume_threshold} DTE\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d6430d-fc83-4eb6-aaa9-25c1fb1901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of normalized values given an ndarray of nominal values\n",
    "def normalize_nd_array(to_normalize):\n",
    "    '''train the normalization'''\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(to_normalize)\n",
    "    # print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "    # normalize the dataset and print the first 5 rows\n",
    "    normalized = scaler.transform(to_normalize)\n",
    "    normalized = list(map(lambda x: x[0], normalized.tolist()))\n",
    "    # for i in range(5):\n",
    "    # \tprint(normalized[i])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5ca5e69-b4fb-4c17-973b-d3995fdcb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes_for_volume_grouped_by_minute(df_volume_by_minute, unique_trading_days):\n",
    "    '''\n",
    "    Create and populate an array of dataframes. Each dataframe contains one trading days worth of volume values grouped by minute\n",
    "    and normalized against just that days worth of activity\n",
    "    '''\n",
    "    frames = []\n",
    "    for i in trange(len(unique_trading_days), desc=\"Splitting into dataframes grouped by minute\"):\n",
    "        day=unique_trading_days[i]\n",
    "        string_date = day.strftime(\"%Y-%m-%d\")\n",
    "        days_df = df_volume_by_minute.loc[string_date].copy()\n",
    "        volume_values = days_df['Volume'].values\n",
    "        volume_values = volume_values.reshape((len(volume_values), 1))\n",
    "        normalized_day_volume = normalize_nd_array(volume_values)\n",
    "        days_df['Volume Normalized Intraday'] = normalized_day_volume\n",
    "        frames.append(days_df.copy())\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edfb4991-64ff-4d0a-b392-edb71ecad5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_single_df(frames):\n",
    "    '''Concatenate the entire array of dataframes back into one big dataframe that contains the Volume Normalized Intraday values for every minute of every day'''\n",
    "    df_intraday_normalized = pd.concat(frames)\n",
    "    return df_intraday_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2343869e-1c4c-4c59-839e-8667b435279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the volume Normalized intraday Avg\n",
    "def calculate_normalized_vol_by_minute(intraday_summed, num_unique_trading_days):\n",
    "    return intraday_summed / num_unique_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210f867b-9783-466e-a687-ec1068bab67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_minute_sum_normalized_volumes(df_intraday_normalized, num_unique_trading_days):\n",
    "    '''Group by minute across all days summing the intraday normalized volumes'''\n",
    "    df_normalized_grouped_by_minute = df_intraday_normalized.groupby(lambda x: x.time()).sum().rename(columns={'Volume Normalized Intraday':'Volume Normalized Summed'})\n",
    "    # Add a column that shows the average intraday normalized volume for each minute\n",
    "    df_normalized_grouped_by_minute['Daily Avg Volume Normalized'] = df_normalized_grouped_by_minute.apply(\n",
    "        lambda row: calculate_normalized_vol_by_minute(row['Volume Normalized Summed'],\n",
    "                                                       num_unique_trading_days),\n",
    "        axis=1\n",
    "    )\n",
    "    return df_normalized_grouped_by_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb243628-b227-47ef-84ee-d83bce2cb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dte_for_row(row, last_unique_trading_minute_in_contract):\n",
    "    this_row_date = row.name.to_pydatetime()\n",
    "    # print(f\"this_row_date {this_row_date}\")\n",
    "    time_difference = last_unique_trading_minute_in_contract - this_row_date\n",
    "    # print(f\"time_difference {time_difference}\")\n",
    "    return time_difference.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbf706c3-b8c2-4ed3-9329-580bc01ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dte_column_to_df(a_contract_df):\n",
    "    unique_trading_days = list(get_unique_trading_days(a_contract_df))\n",
    "    unique_trading_days.sort()\n",
    "    last_unique_trading_day_in_contract = unique_trading_days[-1].strftime(\"%Y-%m-%d\")\n",
    "    last_unique_trading_minute_in_contract = a_contract_df.loc[last_unique_trading_day_in_contract].iloc[-1].name.to_pydatetime()\n",
    "    a_contract_df[\"Days To Contract Expiration\"] = a_contract_df.apply(lambda r: calculate_dte_for_row(r, last_unique_trading_minute_in_contract), axis=1)\n",
    "    return a_contract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba44504-604c-4737-b75a-633cb6ea4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master_ungrouped_data_frame(files_to_process):\n",
    "    '''Build up a single dataframe containing volume DateTime and DTE for all contracts'''\n",
    "    grouped_df = pd.DataFrame(columns = [\"DateTime\", \"Volume\"]).set_index('DateTime')\n",
    "    for i in trange(len(files_to_process), desc=f\"Overall Analysis\"):\n",
    "        file = files_to_process[i]\n",
    "        contract_symbol = file[:len(file) - 4]\n",
    "        a_contract_df = convert_csv_to_df(file)\n",
    "        with_dte_df = add_dte_column_to_df(a_contract_df)\n",
    "        grouped_df = combine_data_frames(grouped_df, with_dte_df)\n",
    "        # display(a_contract_df)\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd260140-d363-490c-8f71-30b86327b19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde379bb926848b09c78c622cd4aca12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Analysis:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather all the data from every contract into one big dataframe\n",
    "master_ungrouped_df = get_master_ungrouped_data_frame(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace73875-61e8-49aa-b0bb-ebaa8b8aa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the ungrouped dataframe\n",
    "master_ungrouped_df = master_ungrouped_df.sort_values(by=['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a179e452-aacc-4a84-8782-22d26c8682c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe in two based on the DTE threshold\n",
    "less_than_or_equal_dte_threshold_df, greater_than_dte_threshold_df = split_dataframe_by_dte(master_ungrouped_df, DAYS_TO_EXPIRATION_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9ee4f98-c440-4c95-ac3c-344c7a86cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the unique trading days before and after the DTE threshold\n",
    "less_than_or_equal_dte_unique_trading_days = get_unique_trading_days(less_than_or_equal_dte_threshold_df)\n",
    "greater_than_dte_unique_trading_days = get_unique_trading_days(greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9527a544-893f-4310-bc36-28d42e91a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique trading days before and after the DTE threshold\n",
    "num_lte_dte_unique_trading_days = len(less_than_or_equal_dte_unique_trading_days)\n",
    "num_gt_dte_unique_trading_days = len(greater_than_dte_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0702f-670e-4bd8-91c4-05f76904b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the before and after dataframes to show the total volume by minute of the day\n",
    "lte_dte_by_minute_df = resample_volume_by_minute(less_than_or_equal_dte_threshold_df)\n",
    "gt_dte_by_minute_df = resample_volume_by_minute(greater_than_dte_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dae454-ba8c-40e0-beb6-034361997579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average intraday volume for each minute of the day before and after the dte threshold\n",
    "# Put all that information into one dataframe for charting\n",
    "master_avg_daily_nominal_df = get_master_avg_daily_nominal_df(\n",
    "    lte_dte_by_minute_df, gt_dte_by_minute_df,\n",
    "    num_lte_dte_unique_trading_days, num_gt_dte_unique_trading_days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4355a-1576-4eae-a0bf-68c76e346f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from the dataframe that we have no intention of charting\n",
    "master_avg_daily_nominal_df = clean_master_avg_daily_nominal_df(\n",
    "    master_avg_daily_nominal_df, DAYS_TO_EXPIRATION_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6f3e-745f-4073-a20c-725a5f0eb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two arrays of dataframes each array element is a dataframe for a single days worth of intraday normalized volume\n",
    "lte_dte_threshold_frames = create_dataframes_for_volume_grouped_by_minute(less_than_or_equal_dte_threshold_df, less_than_or_equal_dte_unique_trading_days)\n",
    "gt_dte_threshold_frames = create_dataframes_for_volume_grouped_by_minute(greater_than_dte_threshold_df, greater_than_dte_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1674c44-2c62-4b7a-87b6-aa9a9db469e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the array of dataframes into one big dataframe each for before and after dte threshold date\n",
    "df_intraday_normalized_lte_dte = concat_to_single_df(lte_dte_threshold_frames)\n",
    "df_intraday_normalized_gt_dte = concat_to_single_df(gt_dte_threshold_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e34a-b382-45da-b8f2-43596296570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by minute normalized for both before and after the cutoff date\n",
    "df_intraday_normalized_lte_dte_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_lte_dte, num_lte_dte_unique_trading_days)\n",
    "df_intraday_normalized_gt_grouped_by_minute = group_by_minute_sum_normalized_volumes(df_intraday_normalized_gt_dte, num_gt_dte_unique_trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc114f45-576d-4077-b728-39251437fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and drop some columns we no longer need\n",
    "df_intraday_normalized_lte_dte_grouped_by_minute = df_intraday_normalized_lte_dte_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume <= {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "df_intraday_normalized_lte_dte_grouped_by_minute = df_intraday_normalized_lte_dte_grouped_by_minute.drop([\"Volume Normalized Summed\", \"Days To Contract Expiration\"], axis=1)\n",
    "df_intraday_normalized_gt_grouped_by_minute = df_intraday_normalized_gt_grouped_by_minute.rename(columns={'Daily Avg Volume Normalized':f\"Average Volume > {DAYS_TO_EXPIRATION_THRESHOLD} DTE\"})\n",
    "df_intraday_normalized_gt_grouped_by_minute = df_intraday_normalized_gt_grouped_by_minute.drop([\"Volume Normalized Summed\", \"Days To Contract Expiration\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24ce22-eb76-49fd-b073-d55cb4a18dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all relevant normalized data points\n",
    "master_avg_daily_normalized_df = pd.concat([df_intraday_normalized_lte_dte_grouped_by_minute, df_intraday_normalized_gt_grouped_by_minute], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540059-a4e0-4952-9e29-35075dba140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display our figures for nominal and normalized intraday volume\n",
    "fig1 = master_avg_daily_nominal_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Nominal Trading Volume By Minute\")\n",
    "fig2 = master_avg_daily_normalized_df.plot(kind=\"line\", title=f\"All Contracts Starting With {CONTRACTS_PREFIX_MATCHER} - Average Intraday Normalized Trading Volume By Minute\")\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25f525-0dd4-4195-bd95-06a0365c14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
